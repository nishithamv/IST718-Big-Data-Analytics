{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grading Feedback Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": false,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Willard Williamson <wewillia@syr.edu>\n",
    "- Faculty Assistant: Yash Pasar\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- __Do not change homework file names.__ The FAs and the professor use these names to grade your homework.  Changing file names may result in a point reduction penalty.\n",
    "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
    "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
    "- All plots shall include a title, and axis labels.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: MapReduce with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa1d6ce6f6b1b683217a308c184d09b9",
     "grade": false,
     "grade_id": "cell-bd77448c7e20b596",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to create the Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import types\n",
    "import numpy as np\n",
    "import numpy.testing as testing\n",
    "spark = SparkSession.builder.appName('map-reduce').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this set of questions, we will study 10 stocks during one year (365 days). The data that we have are the log returns of the stocks.\n",
    "\n",
    "The log return of a stock from time $t-1$ to time $t$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "r_t &= \\log (\\frac{P_t}{P_{t-1}})\\\\\n",
    "    &= \\log (P_t) - \\log(P_{t-1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\log$ is the natural log and $P_t$ is the price of the stock at time $t$.\n",
    "\n",
    "For example, if the stock price of a stock is 100 on day 0 and 120 on day 1, then the log return at time 1 is $\\log(120) - \\log(100) = 0.18323$. Positive log return means an increase in price and negative log return means a decrease in price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 Read data (5 pts)\n",
    "\n",
    "In this question, you will transform an RDD that contains the contents of a text file with the prices of 10 stocks over one year. Each element of this RDD has the format \"time t\", \"stock ticker\", and \"return at time t\". For example:\n",
    "\n",
    "```python\n",
    "stock_returns_rdd.take(5)\n",
    "```\n",
    "\n",
    "```\n",
    "['1    FAD   0.044',\n",
    " '1    DHJ   0.033',\n",
    " '1    DFC   0.149',\n",
    " '1    EHG  -0.021',\n",
    " '1    IIK   0.031']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1516dcd0b24d22fd856043d546e0b018",
     "grade": false,
     "grade_id": "cell-998fe768577cf480",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# upload the hw2_log_stock_returns.txt data file to data bricks and read it in here\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "\n",
    "def get_training_filename(data_file_name):    \n",
    "    \n",
    "    grading_env = os.getenv(\"GRADING_RUNTIME_ENV\")\n",
    "    \n",
    "    if db_env != None:\n",
    "\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "\n",
    "    elif grading_env != None:\n",
    "\n",
    "        full_path_name = \"%s/%s\" % (grading_env, data_file_name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "\n",
    "    return full_path_name\n",
    "\n",
    "stock_returns_rdd = sc.textFile(get_training_filename('hw2_log_stock_returns.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1    FAD   0.044',\n",
       " '1    DHJ   0.033',\n",
       " '1    DFC   0.149',\n",
       " '1    EHG  -0.021',\n",
       " '1    IIK   0.031']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "stock_returns_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spark, transform `stock_returns_rdd` into another RDD where each element is a list of three elements `[time, ticker, log return]`. `time`, `ticker`, and `log return` must be an integer, string, and floating point number, respectively. You should define a map function `map_stock_list` to apply to `stock_returns_rdd`. Call this new rdd `stock_list_rdd`.\n",
    "\n",
    "```python\n",
    "stock_list_rdd.take(5)\n",
    "```\n",
    "\n",
    "```\n",
    "[[1, 'FAD', 0.044],\n",
    " [1, 'DHJ', 0.033],\n",
    " [1, 'DFC', 0.149],\n",
    " [1, 'EHG', -0.021],\n",
    " [1, 'IIK', 0.031]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a166902da626bb23d7587ef4c594dd54",
     "grade": false,
     "grade_id": "cell-9cd285edff34dc29",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'FAD', 0.044],\n",
       " [1, 'DHJ', 0.033],\n",
       " [1, 'DFC', 0.149],\n",
       " [1, 'EHG', -0.021],\n",
       " [1, 'IIK', 0.031]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to transform each element of stock_returns_rdd and then define stock_list_rdd\n",
    "def map_stock_list(e):\n",
    "    x = e.split()\n",
    "    \n",
    "    return [int(x[0]), x[1], float(x[2])]\n",
    "\n",
    "stock_list_rdd = stock_returns_rdd.map(map_stock_list)\n",
    "\n",
    "stock_list_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff69b66fa6d935362b774a56fed1a7b7",
     "grade": true,
     "grade_id": "cell-cb3fc3eeaf78b636",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 5 pts: check that stock_list_rdd was created properly\n",
    "testing.assert_array_less([stock_returns_rdd.id()], [stock_list_rdd.id()])\n",
    "testing.assert_equal(stock_list_rdd.first(), [1, 'FAD', 0.044])\n",
    "testing.assert_equal(stock_list_rdd.count(), 3650)\n",
    "testing.assert_almost_equal(stock_list_rdd.map(lambda x: x[-1]).sum(), \n",
    "                           -9.597000000000007, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.2 (10 pts) Maximum, minimum, and total:\n",
    "\n",
    "You will now create three map-reduce jobs that will produce RDDs that contain 1) the maximum log return per stock (`max_return_rdd`), 2) the minimum log return per stock (`min_return_rdd`), and 3) the total (sum) log return per stock (`total_return_rdd`) for the year. The map-reduce starts from `stock_list_rdd` defined in question 1.1. You should define the map reduce function for each RDD `map_maximum`, `reduce_maximum`, `map_minimum`, `reduce_minimum`, `map_total`, and `reduce_total`.\n",
    "\n",
    "Each of these RDDs should have key-value pair elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7150285e4311950e788ac86dd66f0cbb",
     "grade": false,
     "grade_id": "cell-3add8de5f50ca372",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define map, reduce, and rdd for maximum\n",
    "def map_maximum(e):\n",
    "    \n",
    "    return (e[1], e[2])\n",
    "    \n",
    "def reduce_maximum(v1, v2):\n",
    "    \n",
    "    return max(v1, v2)\n",
    "\n",
    "# define rdd below\n",
    "max_return_rdd = stock_list_rdd.map(map_maximum).reduceByKey(reduce_maximum)\n",
    "\n",
    "# define map, reduce, and rdd for minimum\n",
    "def map_minimum(e):\n",
    "    \n",
    "    return (e[1], e[2])\n",
    "    \n",
    "def reduce_minimum(v1, v2):\n",
    "   \n",
    "    return min(v1, v2)\n",
    "\n",
    "# define rdd below\n",
    "min_return_rdd = stock_list_rdd.map(map_minimum).reduceByKey(reduce_minimum)\n",
    "\n",
    "# define map, reduce, and rdd for total\n",
    "def map_total(e):\n",
    "    \n",
    "    return (e[1], e[2])\n",
    "    \n",
    "def reduce_total(v1, v2):\n",
    "\n",
    "    return v1 + v2\n",
    "\n",
    "# define rdd below\n",
    "total_return_rdd = stock_list_rdd.map(map_total).reduceByKey(reduce_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6ffb4f21c5cbecb8c2a0eff62689df0b",
     "grade": true,
     "grade_id": "cell-0f2d328c036f9931",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 points\n",
    "testing.assert_equal(type(map_maximum), types.FunctionType)\n",
    "testing.assert_equal(type(reduce_maximum), types.FunctionType)\n",
    "testing.assert_equal(type(map_minimum), types.FunctionType)\n",
    "testing.assert_equal(type(reduce_minimum), types.FunctionType)\n",
    "testing.assert_equal(type(map_total), types.FunctionType)\n",
    "testing.assert_equal(type(reduce_total), types.FunctionType)\n",
    "\n",
    "\n",
    "testing.assert_array_almost_equal(max_return_rdd.sortByKey().values().collect(), \n",
    "                              [0.276, 0.238, 0.262, 0.241, 0.343, 0.317, 0.264, 0.293, 0.38, 0.331], decimal=2)\n",
    "\n",
    "testing.assert_array_almost_equal(min_return_rdd.sortByKey().values().collect(), \n",
    "                              [-0.258,\n",
    " -0.305,\n",
    " -0.299,\n",
    " -0.283,\n",
    " -0.274,\n",
    " -0.283,\n",
    " -0.277,\n",
    " -0.281,\n",
    " -0.312,\n",
    " -0.301], decimal=2)\n",
    "\n",
    "testing.assert_array_almost_equal(total_return_rdd.sortByKey().values().collect(),\n",
    "                              [-2.1779999999999995,\n",
    " 3.1999999999999984,\n",
    " -0.36000000000000054,\n",
    " -1.2189999999999996,\n",
    " 0.3419999999999993,\n",
    " -2.747,\n",
    " -0.515999999999999,\n",
    " -4.391000000000002,\n",
    " -0.9330000000000009,\n",
    " -0.7949999999999992], decimal=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.4 (20 pts)\n",
    "\n",
    "We can compute the total log return during a period $t_1$ and $t_2$ ($t_1 < t_2$) by using the following derivation\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{t_1 : t_2} &= \\log \\frac{P_{t_2}}{P_{t_1}}\\\\\n",
    "              &= \\log \\frac{P_{t_2}}\n",
    "                           {P_{t_1}}\n",
    "                       \\frac{P_{t_2-1} P_{t_2-2}\\cdots P_{t_1+1}}\n",
    "                           {P_{t_2-1} P_{t_2-2} P_{t_2-3}\\cdots P_{t_1+1}}\n",
    "\\end{align}\n",
    "$$\n",
    "then re-arranging\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{t_1 : t_2} &= \\log \\frac{P_{t_2}}\n",
    "                           {P_{t_2-1}}\n",
    "                       \\frac{P_{t_2-1}}\n",
    "                           {P_{t_2-2}}\n",
    "                       \\cdots\n",
    "                       \\frac{P_{t_1+2}}\n",
    "                           {P_{t_1+1}}\n",
    "                       \\frac{P_{t_1+1}}\n",
    "                           {P_{t_1}}\\\\\n",
    "              &= \\log \\frac{P_{t_2}}\n",
    "                           {P_{t_2-1}} +\n",
    "                 \\log \\frac{P_{t_2-1}}\n",
    "                           {P_{t_2-2}} +\n",
    "                       \\cdots +\n",
    "                 \\log \\frac{P_{t_1+2}}\n",
    "                           {P_{t_1+1}} +\n",
    "                 \\log \\frac{P_{t_1+1}}\n",
    "                           {P_{t_1}}  \\\\\n",
    "              &= r_{t_2} + r_{t_2-1} + \\cdots + r_{t_1+2} + r_{t_1+1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "r_{t_1 : t_2} = \\sum_{i=t_1+1}^{t_2} r_i\n",
    "$$\n",
    "\n",
    "\n",
    "Generate an RDD that would contain the `[(ticker, time), cumulative return up to time)]` where the cumulative return up to time $t$ equals $r_{0:t}$. Below define the functions `map_cumulative` and `reduce_cumulative` and store the resulting RDD in `cumulative_return_rdd2`. \n",
    "**Hint: You only need `stock_list_rdd` to generate these results and no join is necessary. The map step in this map-reduce should generate a list of key-value pairs and therefore you need a `flatMap` instead of a `map` operation**. Assume that you know that the maximum time is $t = 365$.\n",
    "\n",
    "For example, we know that the returns of stock \"ADF\" are:\n",
    "\n",
    "```python\n",
    "stock_list_rdd.filter(lambda x: x[1] == 'ADF').take(10)\n",
    "```\n",
    "\n",
    "```\n",
    "[[1, 'ADF', -0.074],\n",
    " [2, 'ADF', -0.198],\n",
    " [3, 'ADF', 0.195],\n",
    " [4, 'ADF', -0.118],\n",
    " [5, 'ADF', -0.173],\n",
    " [6, 'ADF', -0.123],\n",
    " [7, 'ADF', -0.154],\n",
    " [8, 'ADF', 0.098],\n",
    " [9, 'ADF', 0.097],\n",
    " [10, 'ADF', 0.191]]\n",
    "```\n",
    "\n",
    "Therefore, the map-reduce result for \"ADF\" should contain\n",
    "\n",
    "```python\n",
    "(stock_list_rdd.\n",
    " filter(lambda x: x[1] == 'ADF').\n",
    " flatMap(map_cumulative).\n",
    " reduceByKey(reduce_cumulative).sortByKey().take(10)\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "[(('ADF', 1), -0.074),\n",
    " (('ADF', 2), -0.272),\n",
    " (('ADF', 3), -0.07700000000000001),\n",
    " (('ADF', 4), -0.195),\n",
    " (('ADF', 5), -0.368),\n",
    " (('ADF', 6), -0.491),\n",
    " (('ADF', 7), -0.645),\n",
    " (('ADF', 8), -0.547),\n",
    " (('ADF', 9), -0.45000000000000007),\n",
    " (('ADF', 10), -0.25900000000000006)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "654bd2ab53142bb7c267afc8edb98bc8",
     "grade": false,
     "grade_id": "cell-4c9a2170c22b0425",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def map_cumulative(e):\n",
    "    \n",
    "    return [((e[1], time), e[2]) for time in range(e[0], 366)]\n",
    "\n",
    "def reduce_cumulative(v1, v2):\n",
    "    \n",
    "    return v1 +v2\n",
    "    \n",
    "cumulative_return_rdd2 = stock_list_rdd.flatMap(map_cumulative).reduceByKey(reduce_cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.074,\n",
       " -0.272,\n",
       " -0.07700000000000001,\n",
       " -0.195,\n",
       " -0.368,\n",
       " -0.491,\n",
       " -0.645,\n",
       " -0.547,\n",
       " -0.45000000000000007,\n",
       " -0.25900000000000006]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it here\n",
    "cumulative_return_rdd2.filter(lambda x: x[0][0] == 'ADF').sortByKey().values().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "149f63d10d79f3d4fda31d1d02d34e48",
     "grade": true,
     "grade_id": "cell-e77d60d03d3666fe",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 20 pts\n",
    "testing.assert_equal(type(map_cumulative), types.FunctionType)\n",
    "testing.assert_equal(type(reduce_cumulative), types.FunctionType)\n",
    "testing.assert_equal(cumulative_return_rdd2.count(), 3650)\n",
    "testing.assert_equal(cumulative_return_rdd2.first(), (('FAD', 1), 0.044))\n",
    "testing.assert_array_almost_equal(\n",
    "    cumulative_return_rdd2.filter(lambda x: x[0][0] == 'ADF').sortByKey().values().take(10),\n",
    "    [-0.074,\n",
    " -0.272,\n",
    " -0.07700000000000001,\n",
    " -0.195,\n",
    " -0.368,\n",
    " -0.491,\n",
    " -0.645,\n",
    " -0.547,\n",
    " -0.45000000000000007,\n",
    " -0.25900000000000006], decimal=2\n",
    ")\n",
    "\n",
    "testing.assert_array_almost_equal(\n",
    "    cumulative_return_rdd2.filter(lambda x: x[0][0] == 'FAD').sortByKey().values().take(10),\n",
    "    [0.044,\n",
    " 0.271,\n",
    " 0.23600000000000002,\n",
    " 0.18500000000000003,\n",
    " 0.18200000000000002,\n",
    " 0.2,\n",
    " 0.24000000000000002,\n",
    " 0.389,\n",
    " 0.425,\n",
    " 0.308], decimal=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
